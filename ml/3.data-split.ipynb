{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723409f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5940d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ No duplicate samples between train and test sets: AM-I-filtered/AM-I-filtered_with_labels_k4.csv\n",
      "Saved train data for AM-I-filtered/AM-I-filtered_with_labels_k4.csv to ./train_test_split/AM-I-filtered_with_labels_k4_train.csv\n",
      "Saved test data for AM-I-filtered/AM-I-filtered_with_labels_k4.csv to ./train_test_split/AM-I-filtered_with_labels_k4_test.csv\n",
      "‚úÖ No duplicate samples between train and test sets: AM-II-filtered/AM-II-filtered_with_labels_k3.csv\n",
      "Saved train data for AM-II-filtered/AM-II-filtered_with_labels_k3.csv to ./train_test_split/AM-II-filtered_with_labels_k3_train.csv\n",
      "Saved test data for AM-II-filtered/AM-II-filtered_with_labels_k3.csv to ./train_test_split/AM-II-filtered_with_labels_k3_test.csv\n",
      "‚úÖ No duplicate samples between train and test sets: AM-III-filtered/AM-III-filtered_with_labels_k4.csv\n",
      "Saved train data for AM-III-filtered/AM-III-filtered_with_labels_k4.csv to ./train_test_split/AM-III-filtered_with_labels_k4_train.csv\n",
      "Saved test data for AM-III-filtered/AM-III-filtered_with_labels_k4.csv to ./train_test_split/AM-III-filtered_with_labels_k4_test.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set file paths\n",
    "data_dir = \"./clustering\"  # Directory containing clustering results\n",
    "output_dir = \"./train_test_split\"  # Directory to save train-test split results\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# File list\n",
    "files = [\n",
    "    \"AM-I-filtered/AM-I-filtered_with_labels_k4.csv\",\n",
    "    'AM-II-filtered/AM-II-filtered_with_labels_k3.csv',\n",
    "    'AM-III-filtered/AM-III-filtered_with_labels_k4.csv'\n",
    "]\n",
    "\n",
    "# Random seed\n",
    "random_seed = 42\n",
    "\n",
    "# Main process\n",
    "for file in files:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)  # Read clustering result file\n",
    "\n",
    "    # Remove rows with missing values\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Initialize empty DataFrames to store train and test data for the current file\n",
    "    train_data = pd.DataFrame()\n",
    "    test_data = pd.DataFrame()\n",
    "\n",
    "    # Group by UMAP_Cluster\n",
    "    for cluster in df['UMAP_Cluster'].unique():\n",
    "        cluster_df = df[df['UMAP_Cluster'] == cluster].reset_index(drop=True)\n",
    "\n",
    "        # Split into train and test sets\n",
    "        train_df, test_df = train_test_split(cluster_df, test_size=0.1, random_state=random_seed)\n",
    "\n",
    "        # Append current cluster's train and test data to the main DataFrames\n",
    "        train_data = pd.concat([train_data, train_df], ignore_index=True)\n",
    "        test_data = pd.concat([test_data, test_df], ignore_index=True)\n",
    "\n",
    "    # Check for duplicate samples between train and test sets\n",
    "    duplicate_rows = train_data.merge(test_data, how='inner')\n",
    "    if not duplicate_rows.empty:\n",
    "        print(f\"‚ö†Ô∏è Potential data leakage detected (duplicate samples in train and test sets): {file}\")\n",
    "        print(f\"Number of duplicate samples = {len(duplicate_rows)}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ No duplicate samples between train and test sets: {file}\")\n",
    "\n",
    "    # Create output filenames (remove directory path, keep only filename)\n",
    "    base_filename = os.path.basename(file_path)  # Get filename, e.g., \"Default-2_with_labels_k4.csv\"\n",
    "    base_name_without_ext = os.path.splitext(base_filename)[0]  # Remove extension\n",
    "    \n",
    "    # Save train and test sets for the current file\n",
    "    train_output_path = os.path.join(output_dir, f\"{base_name_without_ext}_train.csv\")\n",
    "    test_output_path = os.path.join(output_dir, f\"{base_name_without_ext}_test.csv\")\n",
    "\n",
    "    train_data.to_csv(train_output_path, index=False)\n",
    "    test_data.to_csv(test_output_path, index=False)\n",
    "\n",
    "    print(f\"Saved train data for {file} to {train_output_path}\")\n",
    "    print(f\"Saved test data for {file} to {test_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da57eafb",
   "metadata": {},
   "source": [
    "comparison of chemical space and re distribution between test and train set in AM-I, AM-II, AM-III dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03b211f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:48:36 [INFO] üìÅ Found 3 training files\n",
      "15:48:36 [INFO] üîÑ Starting analysis of 3 datasets...\n",
      "15:48:36 [INFO] ==================================================\n",
      "15:48:36 [INFO] üìä Starting retention time distribution analysis\n",
      "15:48:36 [INFO] ==================================================\n",
      "15:48:36 [INFO] üîç Analyzing retention time distribution: AM-I-filtered_with_labels_k4_train.csv vs AM-I-filtered_with_labels_k4_test.csv\n",
      "15:48:38 [INFO] ‚úÖ Retention time analysis completed: AM-I-filtered_with_labels_k4\n",
      "15:48:38 [INFO] üîç Analyzing retention time distribution: AM-II-filtered_with_labels_k3_train.csv vs AM-II-filtered_with_labels_k3_test.csv\n",
      "15:48:39 [INFO] ‚úÖ Retention time analysis completed: AM-II-filtered_with_labels_k3\n",
      "15:48:39 [INFO] üîç Analyzing retention time distribution: AM-III-filtered_with_labels_k4_train.csv vs AM-III-filtered_with_labels_k4_test.csv\n",
      "15:48:39 [INFO] ‚úÖ Retention time analysis completed: AM-III-filtered_with_labels_k4\n",
      "15:48:39 [INFO] ‚úÖ Retention time summary saved to train_test_split/rt-comparison/train_test_rt_summary.csv\n",
      "15:48:39 [INFO] ==================================================\n",
      "15:48:39 [INFO] üî¨ Starting structural distribution analysis (PCA)\n",
      "15:48:39 [INFO] ==================================================\n",
      "15:48:39 [INFO] üß¨ Analyzing structural distribution: AM-I-filtered_with_labels_k4_train.csv vs AM-I-filtered_with_labels_k4_test.csv\n",
      "15:48:40 [INFO] üìä PCA reduced data saved: train_test_split/pca-structure-distribution/pca_reduced_data_AM-I-filtered_with_labels_k4.csv\n",
      "15:48:44 [INFO] üìà PCA plot saved: train_test_split/pca-structure-distribution/pca_plot_AM-I-filtered_with_labels_k4.png\n",
      "15:48:44 [INFO] ‚úÖ Structural analysis completed: AM-I-filtered_with_labels_k4\n",
      "15:48:44 [INFO] üß¨ Analyzing structural distribution: AM-II-filtered_with_labels_k3_train.csv vs AM-II-filtered_with_labels_k3_test.csv\n",
      "15:48:45 [INFO] üìä PCA reduced data saved: train_test_split/pca-structure-distribution/pca_reduced_data_AM-II-filtered_with_labels_k3.csv\n",
      "15:48:47 [INFO] üìà PCA plot saved: train_test_split/pca-structure-distribution/pca_plot_AM-II-filtered_with_labels_k3.png\n",
      "15:48:47 [INFO] ‚úÖ Structural analysis completed: AM-II-filtered_with_labels_k3\n",
      "15:48:47 [INFO] üß¨ Analyzing structural distribution: AM-III-filtered_with_labels_k4_train.csv vs AM-III-filtered_with_labels_k4_test.csv\n",
      "15:48:48 [INFO] üìä PCA reduced data saved: train_test_split/pca-structure-distribution/pca_reduced_data_AM-III-filtered_with_labels_k4.csv\n",
      "15:48:49 [INFO] üìà PCA plot saved: train_test_split/pca-structure-distribution/pca_plot_AM-III-filtered_with_labels_k4.png\n",
      "15:48:49 [INFO] ‚úÖ Structural analysis completed: AM-III-filtered_with_labels_k4\n",
      "15:48:49 [INFO] üéâ All analyses completed!\n",
      "15:48:49 [INFO] üìÅ Retention time results saved in: /home/xuxianyan/uplc/uplc-260116/train_test_split/rt-comparison\n",
      "15:48:49 [INFO] üìÅ Structural analysis results saved in: /home/xuxianyan/uplc/uplc-260116/train_test_split/pca-structure-distribution\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Retention-time distribution comparison and PCA analysis between Train/Test splits.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ---------- Global Constants ----------\n",
    "DATA_FOLDER = Path(\"./train_test_split\")\n",
    "RT_RESULT_FOLDER = Path(\"./train_test_split/rt-comparison\")\n",
    "PCA_RESULT_FOLDER = Path(\"./train_test_split/pca-structure-distribution\")\n",
    "\n",
    "# Create result folders\n",
    "RT_RESULT_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "PCA_RESULT_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TARGET_COL = \"UV_RT-s\"\n",
    "\n",
    "# Feature columns for null value filtering\n",
    "ALL_FEATURES = (\n",
    "    [\"MolWt\", \"logP\", \"TPSA\", \"H_bond_donors\", \"H_bond_acceptors\"]\n",
    "    + [f\"col{i}\" for i in range(823)]\n",
    "    + [f\"fp_{i}\" for i in range(1024)]\n",
    ")\n",
    "\n",
    "# Statistical significance threshold\n",
    "ALPHA = 0.05\n",
    "\n",
    "# ---------- Logging ----------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "\n",
    "# ---------- Plot Style Configuration ----------\n",
    "def _configure_plot_style() -> None:\n",
    "    \"\"\"Global plot styling: fonts, line widths, transparency, etc.\"\"\"\n",
    "    sns.set_theme(style=\"white\")\n",
    "    sns.set_context(\n",
    "        \"paper\",\n",
    "        rc={\n",
    "            \"font.size\": 15,\n",
    "            \"axes.labelsize\": 16,\n",
    "            \"axes.titlesize\": 16,\n",
    "            \"xtick.labelsize\": 14,\n",
    "            \"ytick.labelsize\": 14,\n",
    "            \"legend.fontsize\": 14,\n",
    "            \"lines.linewidth\": 2.5,\n",
    "            \"axes.linewidth\": 1.5,\n",
    "        },\n",
    "    )\n",
    "    plt.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "\n",
    "_configure_plot_style()\n",
    "\n",
    "# ---------- Data I/O ----------\n",
    "def _load_data(train_path: Path, test_path: Path) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Safely read train/test CSV files, filter null values, and return DataFrames.\"\"\"\n",
    "    try:\n",
    "        train_df = pd.read_csv(train_path).dropna(subset=[TARGET_COL] + ALL_FEATURES)\n",
    "        test_df = pd.read_csv(test_path).dropna(subset=[TARGET_COL] + ALL_FEATURES)\n",
    "    except Exception as e:\n",
    "        logging.error(\"Failed to read files: %s\", e)\n",
    "        raise\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def load_fingerprints_from_csv(file_path: Path) -> Optional[np.ndarray]:\n",
    "    \"\"\"Load fingerprint data from a CSV file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        fp_columns = [f'fp_{i}' for i in range(1024)]\n",
    "        if all(col in df.columns for col in fp_columns):\n",
    "            fps = df[fp_columns].values\n",
    "            return fps\n",
    "        else:\n",
    "            logging.warning(f\"Complete fingerprint columns not found in {file_path.name}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ---------- Retention Time Analysis ----------\n",
    "def analyze_retention_time_pair(train_path: Path, test_path: Path) -> Dict:\n",
    "    \"\"\"Analyze retention time distribution for a train/test file pair.\n",
    "    Returns statistical summary and saves KDE plot.\"\"\"\n",
    "    base_name = train_path.stem.replace(\"_train\", \"\")\n",
    "\n",
    "    logging.info(\"üîç Analyzing retention time distribution: %s vs %s\", train_path.name, test_path.name)\n",
    "\n",
    "    train_df, test_df = _load_data(train_path, test_path)\n",
    "    n_train, n_test = len(train_df), len(test_df)\n",
    "\n",
    "    # Descriptive statistics\n",
    "    train_desc = train_df[TARGET_COL].describe()\n",
    "    test_desc = test_df[TARGET_COL].describe()\n",
    "\n",
    "    # Kolmogorov-Smirnov test\n",
    "    ks_stat, p_val = ks_2samp(train_df[TARGET_COL], test_df[TARGET_COL])\n",
    "    ks_reject = p_val < ALPHA\n",
    "\n",
    "    # Plot retention time distribution\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.kdeplot(\n",
    "        train_df[TARGET_COL],\n",
    "        label=\"Train\",\n",
    "        fill=True,\n",
    "        alpha=0.25,\n",
    "        color=\"tab:blue\",\n",
    "    )\n",
    "    sns.kdeplot(\n",
    "        test_df[TARGET_COL],\n",
    "        label=\"Test\",\n",
    "        fill=True,\n",
    "        alpha=0.25,\n",
    "        color=\"tab:orange\",\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Retention Time (s)\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig_path = RT_RESULT_FOLDER / f\"{base_name}_kde.png\"\n",
    "    plt.savefig(fig_path, dpi=600, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    \n",
    "    logging.info(\"‚úÖ Retention time analysis completed: %s\", base_name)\n",
    "\n",
    "    return {\n",
    "        \"dataset\": base_name,\n",
    "        \"train_samples\": n_train,\n",
    "        \"test_samples\": n_test,\n",
    "        \"train_mean\": train_desc[\"mean\"],\n",
    "        \"train_std\": train_desc[\"std\"],\n",
    "        \"test_mean\": test_desc[\"mean\"],\n",
    "        \"test_std\": test_desc[\"std\"],\n",
    "        \"ks_stat\": ks_stat,\n",
    "        \"ks_p_value\": p_val,\n",
    "        \"ks_reject\": ks_reject,\n",
    "    }\n",
    "\n",
    "# ---------- PCA Analysis ----------\n",
    "def perform_pca_analysis(\n",
    "    fps1: np.ndarray, \n",
    "    fps2: np.ndarray, \n",
    "    label1: str, \n",
    "    label2: str, \n",
    "    n_components: int = 2\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Perform PCA dimensionality reduction and save results.\"\"\"\n",
    "    # Check if data is loaded correctly\n",
    "    if fps1 is None or fps2 is None:\n",
    "        logging.error(f\"Data {label1} or {label2} not loaded correctly\")\n",
    "        return None\n",
    "    \n",
    "    if not isinstance(fps1, np.ndarray) or fps1.shape[1] != 1024:\n",
    "        logging.error(f\"Fingerprint data format incorrect for {label1}\")\n",
    "        return None\n",
    "    if not isinstance(fps2, np.ndarray) or fps2.shape[1] != 1024:\n",
    "        logging.error(f\"Fingerprint data format incorrect for {label2}\")\n",
    "        return None\n",
    "    \n",
    "    # Combine both datasets\n",
    "    all_fps = np.vstack([fps1, fps2])\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced = pca.fit_transform(all_fps)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(reduced, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "    df['Label'] = np.concatenate([np.full(len(fps1), label1), np.full(len(fps2), label2)])\n",
    "    \n",
    "    # Save PCA-reduced data\n",
    "    base_name = label1.replace('_train.csv', '')\n",
    "    output_csv_path = PCA_RESULT_FOLDER / f\"pca_reduced_data_{base_name}.csv\"\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    logging.info(\"üìä PCA reduced data saved: %s\", output_csv_path)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_pca(\n",
    "    df: pd.DataFrame, \n",
    "    label1: str, \n",
    "    label2: str\n",
    ") -> None:\n",
    "    \"\"\"Plot PCA results and save figure.\"\"\"\n",
    "    base_name = label1.replace('_train.csv', '')\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = {label1: \"#007AFF\", label2: \"#FFCC00\"}\n",
    "    \n",
    "    sns.scatterplot(\n",
    "        data=df,\n",
    "        x='PC1',\n",
    "        y='PC2',\n",
    "        hue='Label',\n",
    "        palette=colors,\n",
    "        s=110,\n",
    "        alpha=0.7,\n",
    "        linewidth=0.9\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"PCA 1\", fontsize=24)\n",
    "    plt.ylabel(\"PCA 2\", fontsize=24)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(2)\n",
    "    ax.tick_params(width=2, length=10)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "\n",
    "    # Customize legend labels\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    labels = ['train' if l.endswith('_train.csv') else 'test' for l in labels]\n",
    "    \n",
    "    ax.legend(\n",
    "        handles=handles,\n",
    "        labels=labels,\n",
    "        title=None,\n",
    "        fontsize=20,\n",
    "        markerscale=1\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    output_png_path = PCA_RESULT_FOLDER / f\"pca_plot_{base_name}.png\"\n",
    "    plt.savefig(output_png_path, dpi=600)\n",
    "    plt.close()\n",
    "    logging.info(\"üìà PCA plot saved: %s\", output_png_path)\n",
    "\n",
    "\n",
    "def analyze_structure_pair(train_path: Path, test_path: Path) -> None:\n",
    "    \"\"\"Analyze structural distribution (PCA) for a train/test file pair.\"\"\"\n",
    "    base_name = train_path.stem.replace(\"_train\", \"\")\n",
    "    \n",
    "    logging.info(\"üß¨ Analyzing structural distribution: %s vs %s\", train_path.name, test_path.name)\n",
    "    \n",
    "    # Load fingerprint data\n",
    "    fps_train = load_fingerprints_from_csv(train_path)\n",
    "    fps_test = load_fingerprints_from_csv(test_path)\n",
    "    \n",
    "    if fps_train is not None and fps_test is not None:\n",
    "        # Perform PCA analysis\n",
    "        df_pca = perform_pca_analysis(\n",
    "            fps_train, fps_test,\n",
    "            label1=train_path.name,\n",
    "            label2=test_path.name\n",
    "        )\n",
    "        \n",
    "        if df_pca is not None:\n",
    "            # Plot PCA results\n",
    "            plot_pca(df_pca, train_path.name, test_path.name)\n",
    "            logging.info(\"‚úÖ Structural analysis completed: %s\", base_name)\n",
    "    else:\n",
    "        logging.warning(\"‚ö†Ô∏è Skipping structural analysis for %s (fingerprint data missing)\", base_name)\n",
    "\n",
    "# ---------- Main Program ----------\n",
    "def main() -> None:\n",
    "    \"\"\"Main function: execute retention time analysis and structural analysis.\"\"\"\n",
    "    # Check if data folder exists\n",
    "    if not DATA_FOLDER.exists():\n",
    "        logging.error(f\"‚ùå Data folder does not exist: {DATA_FOLDER}\")\n",
    "        logging.info(f\"Please ensure the {DATA_FOLDER} folder exists and contains *_train.csv and *_test.csv files\")\n",
    "        return\n",
    "    \n",
    "    # Get all training files\n",
    "    train_files = sorted(DATA_FOLDER.glob(\"*_train.csv\"))\n",
    "    \n",
    "    if not train_files:\n",
    "        logging.error(\"‚ùå No training files found (*_train.csv)\")\n",
    "        logging.info(f\"Please check if the {DATA_FOLDER} folder contains *_train.csv files\")\n",
    "        return\n",
    "    \n",
    "    logging.info(\"üìÅ Found %d training files\", len(train_files))\n",
    "    \n",
    "    # Build file pairs\n",
    "    pairs: List[Tuple[Path, Path]] = []\n",
    "    for tr in train_files:\n",
    "        te = tr.with_name(tr.name.replace(\"_train.csv\", \"_test.csv\"))\n",
    "        if te.exists():\n",
    "            pairs.append((tr, te))\n",
    "        else:\n",
    "            logging.warning(\"‚ö†Ô∏è Corresponding test file not found: %s\", tr.name)\n",
    "    \n",
    "    if not pairs:\n",
    "        logging.error(\"‚ùå No valid file pairs found\")\n",
    "        logging.info(\"Please ensure each *_train.csv file has a corresponding *_test.csv file\")\n",
    "        return\n",
    "    \n",
    "    logging.info(\"üîÑ Starting analysis of %d datasets...\", len(pairs))\n",
    "    \n",
    "    # Part 1: Retention Time Analysis\n",
    "    logging.info(\"=\" * 50)\n",
    "    logging.info(\"üìä Starting retention time distribution analysis\")\n",
    "    logging.info(\"=\" * 50)\n",
    "    \n",
    "    rt_summary = []\n",
    "    for tr_path, te_path in pairs:\n",
    "        try:\n",
    "            summary = analyze_retention_time_pair(tr_path, te_path)\n",
    "            rt_summary.append(summary)\n",
    "        except Exception as e:\n",
    "            logging.error(\"‚ùå Retention time analysis failed for %s: %s\", tr_path.name, e)\n",
    "    \n",
    "    # Save retention time analysis summary\n",
    "    if rt_summary:\n",
    "        rt_summary_df = pd.DataFrame(rt_summary)\n",
    "        rt_summary_path = RT_RESULT_FOLDER / \"train_test_rt_summary.csv\"\n",
    "        rt_summary_df.to_csv(rt_summary_path, index=False, float_format=\"%.4f\")\n",
    "        logging.info(\"‚úÖ Retention time summary saved to %s\", rt_summary_path)\n",
    "    \n",
    "    # Part 2: Structural Analysis\n",
    "    logging.info(\"=\" * 50)\n",
    "    logging.info(\"üî¨ Starting structural distribution analysis (PCA)\")\n",
    "    logging.info(\"=\" * 50)\n",
    "    \n",
    "    for tr_path, te_path in pairs:\n",
    "        try:\n",
    "            analyze_structure_pair(tr_path, te_path)\n",
    "        except Exception as e:\n",
    "            logging.error(\"‚ùå Structural analysis failed for %s: %s\", tr_path.name, e)\n",
    "    \n",
    "    logging.info(\"üéâ All analyses completed!\")\n",
    "    logging.info(\"üìÅ Retention time results saved in: %s\", RT_RESULT_FOLDER.absolute())\n",
    "    logging.info(\"üìÅ Structural analysis results saved in: %s\", PCA_RESULT_FOLDER.absolute())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab74aad",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyopenms_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
