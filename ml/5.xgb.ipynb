{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e443ad",
   "metadata": {},
   "source": [
    "# xgb for AM-I, AM-II, AM-III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06b5c7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuxianyan/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "XGBoost Sequential Processing for AM Datasets\n",
      "============================================================\n",
      "üìÅ Data folder: /home/xuxianyan/uplc/uplc-260116/train_test_split\n",
      "üìÅ Output folder: /home/xuxianyan/uplc/uplc-260116/xgb-models\n",
      "\n",
      "üìã Files in data folder:\n",
      "   AM-I-filtered_with_labels_k4_test.csv (4.9 MB)\n",
      "   AM-I-filtered_with_labels_k4_train.csv (43.6 MB)\n",
      "   AM-II-filtered_with_labels_k3_test.csv (1.3 MB)\n",
      "   AM-II-filtered_with_labels_k3_train.csv (11.8 MB)\n",
      "   AM-III-filtered_with_labels_k4_test.csv (0.9 MB)\n",
      "   AM-III-filtered_with_labels_k4_train.csv (7.8 MB)\n",
      "\n",
      "========================================\n",
      "\n",
      "üîç Checking data files for AM-I-filtered_with_labels_k4:\n",
      "   Train file: ./train_test_split/AM-I-filtered_with_labels_k4_train.csv\n",
      "   Test file: ./train_test_split/AM-I-filtered_with_labels_k4_test.csv\n",
      "   ‚úÖ Files found. Checking structure...\n",
      "   Train shape: (6118, 1857)\n",
      "   Test shape: (681, 1857)\n",
      "\n",
      "üöÄ Processing dataset: AM-I-filtered_with_labels_k4\n",
      "   üì• Loading data from ./train_test_split...\n",
      "   üìä Data loaded:\n",
      "     Train set: 6118 samples, 1857 columns\n",
      "     Test set: 681 samples, 1857 columns\n",
      "     Target range (train): 33.00 - 112.20\n",
      "     Target range (test): 37.80 - 102.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-19 17:59:40,509] A new study created in memory with name: no-name-cbacaf11-305a-4ae9-b637-df143ac23a64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üîç Optimizing hyperparameters for AM-I-filtered_with_labels_k4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-19 18:48:23,683] Trial 0 finished with value: 0.865777075657072 and parameters: {'max_depth': 8, 'learning_rate': 0.2501250251882702, 'subsample': 0.9387194583773132, 'colsample_bytree': 0.7464807021374643, 'min_child_weight': 1}. Best is trial 0 with value: 0.865777075657072.\n",
      "[I 2026-01-19 18:51:22,647] Trial 14 finished with value: 0.8800862141696306 and parameters: {'max_depth': 9, 'learning_rate': 0.13664501539596258, 'subsample': 0.6764948666394592, 'colsample_bytree': 0.8606829830248302, 'min_child_weight': 5}. Best is trial 14 with value: 0.8800862141696306.\n",
      "[I 2026-01-19 18:58:49,232] Trial 16 finished with value: 0.887855892703365 and parameters: {'max_depth': 7, 'learning_rate': 0.16439288401916352, 'subsample': 0.913790590824245, 'colsample_bytree': 0.8197820036760587, 'min_child_weight': 8}. Best is trial 16 with value: 0.887855892703365.\n",
      "[I 2026-01-19 19:10:43,380] Trial 11 finished with value: 0.8744509370649503 and parameters: {'max_depth': 3, 'learning_rate': 0.07466953350057046, 'subsample': 0.9331809236914408, 'colsample_bytree': 0.7257882433021308, 'min_child_weight': 2}. Best is trial 16 with value: 0.887855892703365.\n",
      "[I 2026-01-19 19:11:19,319] Trial 13 finished with value: 0.8870794061018401 and parameters: {'max_depth': 4, 'learning_rate': 0.2039175826870348, 'subsample': 0.7055873540905754, 'colsample_bytree': 0.9302999449412386, 'min_child_weight': 8}. Best is trial 16 with value: 0.887855892703365.\n",
      "[I 2026-01-19 19:11:33,950] Trial 22 finished with value: 0.39144781309700794 and parameters: {'max_depth': 3, 'learning_rate': 0.0012938250316466794, 'subsample': 0.5741887261438899, 'colsample_bytree': 0.9938358902546304, 'min_child_weight': 3}. Best is trial 16 with value: 0.887855892703365.\n",
      "[I 2026-01-19 19:11:53,892] Trial 4 finished with value: 0.8938051548614672 and parameters: {'max_depth': 4, 'learning_rate': 0.15705009397145514, 'subsample': 0.7409477980043049, 'colsample_bytree': 0.5519270717284646, 'min_child_weight': 8}. Best is trial 4 with value: 0.8938051548614672.\n",
      "[I 2026-01-19 19:12:09,307] Trial 6 finished with value: 0.8438684489958127 and parameters: {'max_depth': 4, 'learning_rate': 0.020338761460973725, 'subsample': 0.5982761039524623, 'colsample_bytree': 0.7905331977127075, 'min_child_weight': 6}. Best is trial 4 with value: 0.8938051548614672.\n",
      "[I 2026-01-19 19:12:10,430] Trial 25 finished with value: 0.6877999087625539 and parameters: {'max_depth': 4, 'learning_rate': 0.0043988765855664845, 'subsample': 0.8835590590733786, 'colsample_bytree': 0.8783089810522157, 'min_child_weight': 5}. Best is trial 4 with value: 0.8938051548614672.\n",
      "[I 2026-01-19 19:12:17,296] Trial 3 finished with value: 0.8955779866127563 and parameters: {'max_depth': 5, 'learning_rate': 0.11210542238901278, 'subsample': 0.7211127078542074, 'colsample_bytree': 0.5310353093552509, 'min_child_weight': 5}. Best is trial 3 with value: 0.8955779866127563.\n",
      "[I 2026-01-19 19:12:31,279] Trial 10 finished with value: 0.8974388912785196 and parameters: {'max_depth': 6, 'learning_rate': 0.07325413857600041, 'subsample': 0.5798119929819086, 'colsample_bytree': 0.5665566691298356, 'min_child_weight': 6}. Best is trial 10 with value: 0.8974388912785196.\n",
      "[I 2026-01-19 19:12:40,831] Trial 21 finished with value: 0.7317710854027223 and parameters: {'max_depth': 5, 'learning_rate': 0.004413171818921544, 'subsample': 0.7417830288568239, 'colsample_bytree': 0.6004178748085784, 'min_child_weight': 8}. Best is trial 10 with value: 0.8974388912785196.\n",
      "[I 2026-01-19 19:12:49,195] Trial 5 finished with value: 0.8962309364912692 and parameters: {'max_depth': 6, 'learning_rate': 0.09680340561724783, 'subsample': 0.7544243199796365, 'colsample_bytree': 0.7182717395145803, 'min_child_weight': 9}. Best is trial 10 with value: 0.8974388912785196.\n",
      "[I 2026-01-19 19:12:52,652] Trial 8 finished with value: 0.6283751109712854 and parameters: {'max_depth': 6, 'learning_rate': 0.0018139081884251669, 'subsample': 0.5442678853531762, 'colsample_bytree': 0.7619202682557379, 'min_child_weight': 5}. Best is trial 10 with value: 0.8974388912785196.\n",
      "[I 2026-01-19 19:12:52,747] Trial 17 finished with value: 0.8941000104568347 and parameters: {'max_depth': 6, 'learning_rate': 0.10257036342771067, 'subsample': 0.8277351142787261, 'colsample_bytree': 0.911826407217835, 'min_child_weight': 4}. Best is trial 10 with value: 0.8974388912785196.\n",
      "[I 2026-01-19 19:12:57,364] Trial 20 finished with value: 0.8892652842613598 and parameters: {'max_depth': 7, 'learning_rate': 0.04382748525318984, 'subsample': 0.9761373536502911, 'colsample_bytree': 0.6715433878308523, 'min_child_weight': 7}. Best is trial 10 with value: 0.8974388912785196.\n",
      "[I 2026-01-19 19:13:12,825] Trial 9 finished with value: 0.6723649181351453 and parameters: {'max_depth': 5, 'learning_rate': 0.002945101853596308, 'subsample': 0.5642436669844431, 'colsample_bytree': 0.6201257146459244, 'min_child_weight': 6}. Best is trial 10 with value: 0.8974388912785196.\n",
      "[I 2026-01-19 19:13:13,804] Trial 19 finished with value: 0.5052767345707749 and parameters: {'max_depth': 5, 'learning_rate': 0.0012877861635987018, 'subsample': 0.7597600254372636, 'colsample_bytree': 0.6999690460381025, 'min_child_weight': 10}. Best is trial 10 with value: 0.8974388912785196.\n",
      "[I 2026-01-19 19:13:26,723] Trial 23 finished with value: 0.8872181904489758 and parameters: {'max_depth': 7, 'learning_rate': 0.030085790617776904, 'subsample': 0.8614094949783311, 'colsample_bytree': 0.6207303460865881, 'min_child_weight': 2}. Best is trial 10 with value: 0.8974388912785196.\n",
      "[I 2026-01-19 19:13:33,520] Trial 12 finished with value: 0.6835084152189472 and parameters: {'max_depth': 9, 'learning_rate': 0.001596520845795488, 'subsample': 0.647042718051155, 'colsample_bytree': 0.9642379455129921, 'min_child_weight': 6}. Best is trial 10 with value: 0.8974388912785196.\n",
      "[I 2026-01-19 19:13:36,082] Trial 2 finished with value: 0.8712253940049046 and parameters: {'max_depth': 7, 'learning_rate': 0.015114147639539, 'subsample': 0.6989475072759921, 'colsample_bytree': 0.5361957858758829, 'min_child_weight': 6}. Best is trial 10 with value: 0.8974388912785196.\n",
      "[I 2026-01-19 19:13:37,654] Trial 15 finished with value: 0.7402749063840146 and parameters: {'max_depth': 9, 'learning_rate': 0.002400756215986027, 'subsample': 0.6170442760198744, 'colsample_bytree': 0.5766210198098156, 'min_child_weight': 8}. Best is trial 10 with value: 0.8974388912785196.\n",
      "[I 2026-01-19 19:13:38,231] Trial 18 finished with value: 0.677064365774245 and parameters: {'max_depth': 7, 'learning_rate': 0.0018730917592142857, 'subsample': 0.6441582015452434, 'colsample_bytree': 0.9886174226702205, 'min_child_weight': 3}. Best is trial 10 with value: 0.8974388912785196.\n",
      "[I 2026-01-19 19:13:40,291] Trial 24 finished with value: 0.5831598955476441 and parameters: {'max_depth': 9, 'learning_rate': 0.0010023104947867842, 'subsample': 0.593673523736184, 'colsample_bytree': 0.8770483669112223, 'min_child_weight': 1}. Best is trial 10 with value: 0.8974388912785196.\n",
      "[I 2026-01-19 19:13:45,066] Trial 7 finished with value: 0.8040399883145019 and parameters: {'max_depth': 10, 'learning_rate': 0.0034245073626360555, 'subsample': 0.8016148944976424, 'colsample_bytree': 0.9509307216366487, 'min_child_weight': 8}. Best is trial 10 with value: 0.8974388912785196.\n",
      "[I 2026-01-19 19:13:46,642] Trial 1 finished with value: 0.687752549725133 and parameters: {'max_depth': 7, 'learning_rate': 0.0020050837776804803, 'subsample': 0.9659005390271469, 'colsample_bytree': 0.7175808587451573, 'min_child_weight': 4}. Best is trial 10 with value: 0.8974388912785196.\n",
      "[I 2026-01-19 19:20:37,403] Trial 26 finished with value: 0.5376345702897896 and parameters: {'max_depth': 5, 'learning_rate': 0.0014489861584281566, 'subsample': 0.7356320439951789, 'colsample_bytree': 0.7526069862404299, 'min_child_weight': 2}. Best is trial 10 with value: 0.8974388912785196.\n",
      "[I 2026-01-19 19:21:04,345] Trial 27 finished with value: 0.8937220590068805 and parameters: {'max_depth': 6, 'learning_rate': 0.06947681064884675, 'subsample': 0.8084893695083104, 'colsample_bytree': 0.5619064690256691, 'min_child_weight': 1}. Best is trial 10 with value: 0.8974388912785196.\n",
      "[I 2026-01-19 19:21:28,454] Trial 28 finished with value: 0.5145060236243302 and parameters: {'max_depth': 4, 'learning_rate': 0.0017532522699490998, 'subsample': 0.978600220327057, 'colsample_bytree': 0.9606557208959448, 'min_child_weight': 1}. Best is trial 10 with value: 0.8974388912785196.\n",
      "[I 2026-01-19 19:22:41,969] Trial 29 finished with value: 0.7340296190865896 and parameters: {'max_depth': 6, 'learning_rate': 0.003238892983546461, 'subsample': 0.9205887680364929, 'colsample_bytree': 0.6213383988391045, 'min_child_weight': 1}. Best is trial 10 with value: 0.8974388912785196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Optuna optimization completed. Best R¬≤: 0.8974\n",
      "   üèãÔ∏è Training final model for AM-I-filtered_with_labels_k4...\n",
      "   üìä Making predictions for AM-I-filtered_with_labels_k4...\n",
      "   üé® Generating plots for AM-I-filtered_with_labels_k4...\n",
      "   üìà Calculating feature importance for AM-I-filtered_with_labels_k4...\n",
      "‚úÖ Completed: AM-I-filtered_with_labels_k4\n",
      "   Test R¬≤: 0.9101, MAE: 2.9681, RMSE: 3.8937\n",
      "   Results saved to: ./xgb-models/AM-I-filtered_with_labels_k4\n",
      "\n",
      "\n",
      "========================================\n",
      "\n",
      "üîç Checking data files for AM-II-filtered_with_labels_k3:\n",
      "   Train file: ./train_test_split/AM-II-filtered_with_labels_k3_train.csv\n",
      "   Test file: ./train_test_split/AM-II-filtered_with_labels_k3_test.csv\n",
      "   ‚úÖ Files found. Checking structure...\n",
      "   Train shape: (1650, 1857)\n",
      "   Test shape: (186, 1857)\n",
      "\n",
      "üöÄ Processing dataset: AM-II-filtered_with_labels_k3\n",
      "   üì• Loading data from ./train_test_split...\n",
      "   üìä Data loaded:\n",
      "     Train set: 1650 samples, 1857 columns\n",
      "     Test set: 186 samples, 1857 columns\n",
      "     Target range (train): 30.60 - 80.40\n",
      "     Target range (test): 32.40 - 76.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-19 19:30:05,064] A new study created in memory with name: no-name-a1e1d5cb-dc01-476d-b9e6-ed6502d813ce\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üîç Optimizing hyperparameters for AM-II-filtered_with_labels_k3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-19 19:54:08,248] Trial 2 finished with value: 0.8252444199166731 and parameters: {'max_depth': 6, 'learning_rate': 0.2844082399756833, 'subsample': 0.7478007350747022, 'colsample_bytree': 0.6238916024151742, 'min_child_weight': 6}. Best is trial 2 with value: 0.8252444199166731.\n",
      "[I 2026-01-19 19:56:06,227] Trial 24 finished with value: 0.8195291225749841 and parameters: {'max_depth': 8, 'learning_rate': 0.20208034607826383, 'subsample': 0.7702147918124977, 'colsample_bytree': 0.7226459507136536, 'min_child_weight': 2}. Best is trial 2 with value: 0.8252444199166731.\n",
      "[I 2026-01-19 20:01:40,414] Trial 19 finished with value: 0.8302393812976133 and parameters: {'max_depth': 9, 'learning_rate': 0.19811563776841498, 'subsample': 0.611698245535932, 'colsample_bytree': 0.9856117256271762, 'min_child_weight': 3}. Best is trial 19 with value: 0.8302393812976133.\n",
      "[I 2026-01-19 20:05:57,072] Trial 4 finished with value: 0.8489726765102503 and parameters: {'max_depth': 4, 'learning_rate': 0.18683318873493765, 'subsample': 0.6517099468929368, 'colsample_bytree': 0.501323170905261, 'min_child_weight': 4}. Best is trial 4 with value: 0.8489726765102503.\n",
      "[I 2026-01-19 20:06:18,098] Trial 5 finished with value: 0.8398771704527549 and parameters: {'max_depth': 10, 'learning_rate': 0.14274709929399057, 'subsample': 0.5161709645736561, 'colsample_bytree': 0.6400171027863386, 'min_child_weight': 2}. Best is trial 4 with value: 0.8489726765102503.\n",
      "[I 2026-01-19 20:07:08,342] Trial 3 finished with value: 0.8446001709986724 and parameters: {'max_depth': 9, 'learning_rate': 0.07628682502276193, 'subsample': 0.6587478444038288, 'colsample_bytree': 0.7658226851858239, 'min_child_weight': 3}. Best is trial 4 with value: 0.8489726765102503.\n",
      "[I 2026-01-19 20:24:53,699] Trial 15 finished with value: 0.8481849570433814 and parameters: {'max_depth': 7, 'learning_rate': 0.18473752790987732, 'subsample': 0.8051229950309432, 'colsample_bytree': 0.5446881543449338, 'min_child_weight': 6}. Best is trial 4 with value: 0.8489726765102503.\n",
      "[I 2026-01-19 20:37:23,898] Trial 23 finished with value: 0.8603208302425394 and parameters: {'max_depth': 4, 'learning_rate': 0.10776023106713749, 'subsample': 0.9215525913280458, 'colsample_bytree': 0.543827520048787, 'min_child_weight': 6}. Best is trial 23 with value: 0.8603208302425394.\n",
      "[I 2026-01-19 20:38:17,206] Trial 7 finished with value: 0.47147760717942255 and parameters: {'max_depth': 3, 'learning_rate': 0.001471402022580092, 'subsample': 0.787107936529823, 'colsample_bytree': 0.7516563225045183, 'min_child_weight': 10}. Best is trial 23 with value: 0.8603208302425394.\n",
      "[I 2026-01-19 20:38:20,922] Trial 8 finished with value: 0.8620989625267137 and parameters: {'max_depth': 3, 'learning_rate': 0.0923806102514422, 'subsample': 0.7205535423519431, 'colsample_bytree': 0.9197552611960879, 'min_child_weight': 2}. Best is trial 8 with value: 0.8620989625267137.\n",
      "[I 2026-01-19 20:38:21,337] Trial 21 finished with value: 0.848395841941701 and parameters: {'max_depth': 5, 'learning_rate': 0.03662340906627402, 'subsample': 0.821097849458045, 'colsample_bytree': 0.6247984673297208, 'min_child_weight': 1}. Best is trial 8 with value: 0.8620989625267137.\n",
      "[I 2026-01-19 20:38:27,451] Trial 6 finished with value: 0.8612186133961905 and parameters: {'max_depth': 4, 'learning_rate': 0.07296825206124938, 'subsample': 0.5392435971577934, 'colsample_bytree': 0.562970393185579, 'min_child_weight': 9}. Best is trial 8 with value: 0.8620989625267137.\n",
      "[I 2026-01-19 20:38:37,408] Trial 14 finished with value: 0.49406092452617256 and parameters: {'max_depth': 4, 'learning_rate': 0.0012537381408485085, 'subsample': 0.8924586415390279, 'colsample_bytree': 0.5923396030871422, 'min_child_weight': 7}. Best is trial 8 with value: 0.8620989625267137.\n",
      "[I 2026-01-19 20:38:40,780] Trial 22 finished with value: 0.44018165464626235 and parameters: {'max_depth': 3, 'learning_rate': 0.0012833274204419106, 'subsample': 0.8466278857013346, 'colsample_bytree': 0.5771337059020258, 'min_child_weight': 9}. Best is trial 8 with value: 0.8620989625267137.\n",
      "[I 2026-01-19 20:38:53,931] Trial 17 finished with value: 0.47146116806799276 and parameters: {'max_depth': 3, 'learning_rate': 0.0013799979967566674, 'subsample': 0.6748563850165012, 'colsample_bytree': 0.7280538427192118, 'min_child_weight': 1}. Best is trial 8 with value: 0.8620989625267137.\n",
      "[I 2026-01-19 20:39:05,072] Trial 1 finished with value: 0.5526170205652199 and parameters: {'max_depth': 5, 'learning_rate': 0.0013532016244407702, 'subsample': 0.7493546947434926, 'colsample_bytree': 0.5336677625293234, 'min_child_weight': 8}. Best is trial 8 with value: 0.8620989625267137.\n",
      "[I 2026-01-19 20:39:05,386] Trial 20 finished with value: 0.8455934922197326 and parameters: {'max_depth': 6, 'learning_rate': 0.06220183968650081, 'subsample': 0.9986704400651438, 'colsample_bytree': 0.8588637163439081, 'min_child_weight': 2}. Best is trial 8 with value: 0.8620989625267137.\n",
      "[I 2026-01-19 20:39:22,693] Trial 11 finished with value: 0.8401125723723423 and parameters: {'max_depth': 8, 'learning_rate': 0.012475592044310618, 'subsample': 0.7186078037058687, 'colsample_bytree': 0.6558105985021682, 'min_child_weight': 4}. Best is trial 8 with value: 0.8620989625267137.\n",
      "[I 2026-01-19 20:39:28,149] Trial 9 finished with value: 0.7770664675115837 and parameters: {'max_depth': 6, 'learning_rate': 0.004469071100458156, 'subsample': 0.7270997850332269, 'colsample_bytree': 0.6330694721878589, 'min_child_weight': 8}. Best is trial 8 with value: 0.8620989625267137.\n",
      "[I 2026-01-19 20:39:35,902] Trial 13 finished with value: 0.8655856943107137 and parameters: {'max_depth': 7, 'learning_rate': 0.07424978577454475, 'subsample': 0.6209333716504447, 'colsample_bytree': 0.5984987014759492, 'min_child_weight': 9}. Best is trial 13 with value: 0.8655856943107137.\n",
      "[I 2026-01-19 20:39:43,225] Trial 12 finished with value: 0.8477327071645046 and parameters: {'max_depth': 9, 'learning_rate': 0.05059357627171217, 'subsample': 0.759649616985241, 'colsample_bytree': 0.9112337222531002, 'min_child_weight': 7}. Best is trial 13 with value: 0.8655856943107137.\n",
      "[I 2026-01-19 20:39:48,269] Trial 10 finished with value: 0.5881891777872614 and parameters: {'max_depth': 9, 'learning_rate': 0.0011975186253818351, 'subsample': 0.7506499419231406, 'colsample_bytree': 0.5302876931303194, 'min_child_weight': 7}. Best is trial 13 with value: 0.8655856943107137.\n",
      "[I 2026-01-19 20:39:53,405] Trial 18 finished with value: 0.8356566623705612 and parameters: {'max_depth': 8, 'learning_rate': 0.00826161123408124, 'subsample': 0.6108275902509721, 'colsample_bytree': 0.5306966225589789, 'min_child_weight': 4}. Best is trial 13 with value: 0.8655856943107137.\n",
      "[I 2026-01-19 20:39:54,927] Trial 0 finished with value: 0.7981859237858997 and parameters: {'max_depth': 8, 'learning_rate': 0.004440004592933647, 'subsample': 0.920278697698709, 'colsample_bytree': 0.6217819358167261, 'min_child_weight': 7}. Best is trial 13 with value: 0.8655856943107137.\n",
      "[I 2026-01-19 20:40:03,713] Trial 25 finished with value: 0.6401678367844432 and parameters: {'max_depth': 8, 'learning_rate': 0.0015775038346901506, 'subsample': 0.7808592092625047, 'colsample_bytree': 0.5859434528390988, 'min_child_weight': 10}. Best is trial 13 with value: 0.8655856943107137.\n",
      "[I 2026-01-19 20:40:17,290] Trial 16 finished with value: 0.7455965523805346 and parameters: {'max_depth': 10, 'learning_rate': 0.0024004664720437313, 'subsample': 0.5469517502829354, 'colsample_bytree': 0.8903402021126536, 'min_child_weight': 5}. Best is trial 13 with value: 0.8655856943107137.\n",
      "[I 2026-01-19 20:43:08,248] Trial 26 finished with value: 0.4798371588267053 and parameters: {'max_depth': 4, 'learning_rate': 0.0011086698240758, 'subsample': 0.7830250486937996, 'colsample_bytree': 0.8512979045242692, 'min_child_weight': 10}. Best is trial 13 with value: 0.8655856943107137.\n",
      "[I 2026-01-19 20:43:46,366] Trial 28 finished with value: 0.845994820511738 and parameters: {'max_depth': 3, 'learning_rate': 0.04247387318089709, 'subsample': 0.531789850975868, 'colsample_bytree': 0.7024599195682785, 'min_child_weight': 8}. Best is trial 13 with value: 0.8655856943107137.\n",
      "[I 2026-01-19 20:44:04,021] Trial 27 finished with value: 0.7512380174940344 and parameters: {'max_depth': 9, 'learning_rate': 0.0025600002883721476, 'subsample': 0.5949471980772134, 'colsample_bytree': 0.5672995329400146, 'min_child_weight': 5}. Best is trial 13 with value: 0.8655856943107137.\n",
      "[I 2026-01-19 20:44:16,632] Trial 29 finished with value: 0.8431714228511917 and parameters: {'max_depth': 5, 'learning_rate': 0.022668070627464298, 'subsample': 0.8740379504110904, 'colsample_bytree': 0.9710274829802652, 'min_child_weight': 3}. Best is trial 13 with value: 0.8655856943107137.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Optuna optimization completed. Best R¬≤: 0.8656\n",
      "   üèãÔ∏è Training final model for AM-II-filtered_with_labels_k3...\n",
      "   üìä Making predictions for AM-II-filtered_with_labels_k3...\n",
      "   üé® Generating plots for AM-II-filtered_with_labels_k3...\n",
      "   üìà Calculating feature importance for AM-II-filtered_with_labels_k3...\n",
      "‚úÖ Completed: AM-II-filtered_with_labels_k3\n",
      "   Test R¬≤: 0.8759, MAE: 2.4358, RMSE: 3.2216\n",
      "   Results saved to: ./xgb-models/AM-II-filtered_with_labels_k3\n",
      "\n",
      "\n",
      "========================================\n",
      "\n",
      "üîç Checking data files for AM-III-filtered_with_labels_k4:\n",
      "   Train file: ./train_test_split/AM-III-filtered_with_labels_k4_train.csv\n",
      "   Test file: ./train_test_split/AM-III-filtered_with_labels_k4_test.csv\n",
      "   ‚úÖ Files found. Checking structure...\n",
      "   Train shape: (1089, 1857)\n",
      "   Test shape: (123, 1857)\n",
      "\n",
      "üöÄ Processing dataset: AM-III-filtered_with_labels_k4\n",
      "   üì• Loading data from ./train_test_split...\n",
      "   üìä Data loaded:\n",
      "     Train set: 1089 samples, 1857 columns\n",
      "     Test set: 123 samples, 1857 columns\n",
      "     Target range (train): 39.60 - 108.60\n",
      "     Target range (test): 42.60 - 103.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-19 20:52:39,094] A new study created in memory with name: no-name-515c7fa7-d2cd-45aa-b22c-6bc1806da482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üîç Optimizing hyperparameters for AM-III-filtered_with_labels_k4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-19 21:11:24,647] Trial 9 finished with value: 0.8216915614314008 and parameters: {'max_depth': 5, 'learning_rate': 0.2727249808080775, 'subsample': 0.9543166694603076, 'colsample_bytree': 0.9669208656075335, 'min_child_weight': 4}. Best is trial 9 with value: 0.8216915614314008.\n",
      "[I 2026-01-19 21:36:35,732] Trial 10 finished with value: 0.8418245356760308 and parameters: {'max_depth': 6, 'learning_rate': 0.09090139928025558, 'subsample': 0.6573879180990411, 'colsample_bytree': 0.9608457970384582, 'min_child_weight': 6}. Best is trial 10 with value: 0.8418245356760308.\n",
      "[I 2026-01-19 21:53:22,134] Trial 20 finished with value: 0.8203811366395986 and parameters: {'max_depth': 10, 'learning_rate': 0.10123508130841984, 'subsample': 0.9985230564098673, 'colsample_bytree': 0.8789375461759092, 'min_child_weight': 10}. Best is trial 10 with value: 0.8418245356760308.\n",
      "[I 2026-01-19 22:01:49,323] Trial 12 finished with value: 0.7835266667393814 and parameters: {'max_depth': 3, 'learning_rate': 0.008750743198040959, 'subsample': 0.5065724338782065, 'colsample_bytree': 0.9627109716810207, 'min_child_weight': 4}. Best is trial 10 with value: 0.8418245356760308.\n",
      "[I 2026-01-19 22:02:29,108] Trial 18 finished with value: 0.8313436086376135 and parameters: {'max_depth': 3, 'learning_rate': 0.030871187430114178, 'subsample': 0.7953707276247486, 'colsample_bytree': 0.9807106954468949, 'min_child_weight': 2}. Best is trial 10 with value: 0.8418245356760308.\n",
      "[I 2026-01-19 22:02:34,419] Trial 4 finished with value: 0.8106567999002827 and parameters: {'max_depth': 3, 'learning_rate': 0.019939055353323635, 'subsample': 0.9256782481628063, 'colsample_bytree': 0.6222995964533551, 'min_child_weight': 5}. Best is trial 10 with value: 0.8418245356760308.\n",
      "[I 2026-01-19 22:02:53,298] Trial 7 finished with value: 0.5224202051305888 and parameters: {'max_depth': 3, 'learning_rate': 0.0011329944891537136, 'subsample': 0.7681364540912068, 'colsample_bytree': 0.9037537601778964, 'min_child_weight': 8}. Best is trial 10 with value: 0.8418245356760308.\n",
      "[I 2026-01-19 22:02:56,385] Trial 0 finished with value: 0.8382403843118469 and parameters: {'max_depth': 5, 'learning_rate': 0.028913616759379302, 'subsample': 0.9359938045069995, 'colsample_bytree': 0.5467507157584989, 'min_child_weight': 7}. Best is trial 10 with value: 0.8418245356760308.\n",
      "[I 2026-01-19 22:03:05,463] Trial 8 finished with value: 0.49893043439364393 and parameters: {'max_depth': 3, 'learning_rate': 0.0010356521973961304, 'subsample': 0.6714184071221492, 'colsample_bytree': 0.6426130753022852, 'min_child_weight': 4}. Best is trial 10 with value: 0.8418245356760308.\n",
      "[I 2026-01-19 22:03:13,827] Trial 11 finished with value: 0.8448068759349802 and parameters: {'max_depth': 5, 'learning_rate': 0.05383258778982797, 'subsample': 0.6840004410701321, 'colsample_bytree': 0.8842556792789049, 'min_child_weight': 9}. Best is trial 11 with value: 0.8448068759349802.\n",
      "[I 2026-01-19 22:03:24,788] Trial 16 finished with value: 0.6166642139835085 and parameters: {'max_depth': 3, 'learning_rate': 0.001878781083800749, 'subsample': 0.9899041393212513, 'colsample_bytree': 0.5217520269892804, 'min_child_weight': 2}. Best is trial 11 with value: 0.8448068759349802.\n",
      "[I 2026-01-19 22:03:28,482] Trial 3 finished with value: 0.8341575497352345 and parameters: {'max_depth': 5, 'learning_rate': 0.027563479756739093, 'subsample': 0.9835730756608883, 'colsample_bytree': 0.6872896104362562, 'min_child_weight': 3}. Best is trial 11 with value: 0.8448068759349802.\n",
      "[I 2026-01-19 22:03:39,097] Trial 15 finished with value: 0.8286289965646451 and parameters: {'max_depth': 7, 'learning_rate': 0.012673267944968791, 'subsample': 0.6622366419051968, 'colsample_bytree': 0.9068879467241004, 'min_child_weight': 5}. Best is trial 11 with value: 0.8448068759349802.\n",
      "[I 2026-01-19 22:03:39,617] Trial 13 finished with value: 0.8045717299335057 and parameters: {'max_depth': 3, 'learning_rate': 0.01517592915536855, 'subsample': 0.6490097540891367, 'colsample_bytree': 0.8488478158001378, 'min_child_weight': 10}. Best is trial 11 with value: 0.8448068759349802.\n",
      "[I 2026-01-19 22:03:44,635] Trial 5 finished with value: 0.7738290111025043 and parameters: {'max_depth': 6, 'learning_rate': 0.0035916225962317905, 'subsample': 0.7928557367443307, 'colsample_bytree': 0.6687852108967289, 'min_child_weight': 9}. Best is trial 11 with value: 0.8448068759349802.\n",
      "[I 2026-01-19 22:03:45,567] Trial 22 finished with value: 0.8291864949761046 and parameters: {'max_depth': 6, 'learning_rate': 0.016442810325035046, 'subsample': 0.8607251173921365, 'colsample_bytree': 0.7701318896978985, 'min_child_weight': 7}. Best is trial 11 with value: 0.8448068759349802.\n",
      "[I 2026-01-19 22:03:59,631] Trial 1 finished with value: 0.7461367655257186 and parameters: {'max_depth': 7, 'learning_rate': 0.0023627405309888778, 'subsample': 0.7738505516839798, 'colsample_bytree': 0.9366689823796726, 'min_child_weight': 9}. Best is trial 11 with value: 0.8448068759349802.\n",
      "[I 2026-01-19 22:04:08,540] Trial 2 finished with value: 0.7799757691646669 and parameters: {'max_depth': 6, 'learning_rate': 0.003628521609296003, 'subsample': 0.9300580658228768, 'colsample_bytree': 0.937440464415462, 'min_child_weight': 2}. Best is trial 11 with value: 0.8448068759349802.\n",
      "[I 2026-01-19 22:04:23,663] Trial 21 finished with value: 0.8053243571078098 and parameters: {'max_depth': 8, 'learning_rate': 0.005509861094633109, 'subsample': 0.9388626881215991, 'colsample_bytree': 0.6360308683903608, 'min_child_weight': 6}. Best is trial 11 with value: 0.8448068759349802.\n",
      "[I 2026-01-19 22:04:26,364] Trial 23 finished with value: 0.8290705854584803 and parameters: {'max_depth': 7, 'learning_rate': 0.013691384127672368, 'subsample': 0.6096144505986851, 'colsample_bytree': 0.5687405171736716, 'min_child_weight': 7}. Best is trial 11 with value: 0.8448068759349802.\n",
      "[I 2026-01-19 22:04:27,355] Trial 25 finished with value: 0.7916532663250548 and parameters: {'max_depth': 7, 'learning_rate': 0.004389452209617339, 'subsample': 0.9813750952931136, 'colsample_bytree': 0.6055781584568886, 'min_child_weight': 8}. Best is trial 11 with value: 0.8448068759349802.\n",
      "[I 2026-01-19 22:04:36,082] Trial 17 finished with value: 0.81087408091459 and parameters: {'max_depth': 8, 'learning_rate': 0.007405015758382748, 'subsample': 0.8481677249742672, 'colsample_bytree': 0.850723810694928, 'min_child_weight': 7}. Best is trial 11 with value: 0.8448068759349802.\n",
      "[I 2026-01-19 22:04:44,217] Trial 24 finished with value: 0.6923659388276109 and parameters: {'max_depth': 8, 'learning_rate': 0.0014054382518164641, 'subsample': 0.945087230328161, 'colsample_bytree': 0.7478835209860046, 'min_child_weight': 1}. Best is trial 11 with value: 0.8448068759349802.\n",
      "[I 2026-01-19 22:04:48,091] Trial 6 finished with value: 0.8188641322531454 and parameters: {'max_depth': 10, 'learning_rate': 0.008034166828849741, 'subsample': 0.6226680561637015, 'colsample_bytree': 0.7960424722371959, 'min_child_weight': 1}. Best is trial 11 with value: 0.8448068759349802.\n",
      "[I 2026-01-19 22:04:49,589] Trial 19 finished with value: 0.7993875629136895 and parameters: {'max_depth': 9, 'learning_rate': 0.003895097432773828, 'subsample': 0.5737194585252381, 'colsample_bytree': 0.6916331720084106, 'min_child_weight': 3}. Best is trial 11 with value: 0.8448068759349802.\n",
      "[I 2026-01-19 22:04:55,390] Trial 14 finished with value: 0.6893598526010777 and parameters: {'max_depth': 8, 'learning_rate': 0.001316210675249189, 'subsample': 0.5652070817798702, 'colsample_bytree': 0.942478633710738, 'min_child_weight': 1}. Best is trial 11 with value: 0.8448068759349802.\n",
      "[I 2026-01-19 22:07:03,641] Trial 26 finished with value: 0.7672237832503628 and parameters: {'max_depth': 3, 'learning_rate': 0.007712371089476896, 'subsample': 0.8373331463610632, 'colsample_bytree': 0.9881593754877163, 'min_child_weight': 5}. Best is trial 11 with value: 0.8448068759349802.\n",
      "[I 2026-01-19 22:10:10,653] Trial 27 finished with value: 0.6193947634735737 and parameters: {'max_depth': 4, 'learning_rate': 0.0014868134037961977, 'subsample': 0.7333945379972917, 'colsample_bytree': 0.551420468820154, 'min_child_weight': 5}. Best is trial 11 with value: 0.8448068759349802.\n",
      "[I 2026-01-19 22:11:39,408] Trial 29 finished with value: 0.8060030693522909 and parameters: {'max_depth': 3, 'learning_rate': 0.01568092367086004, 'subsample': 0.5193002481732909, 'colsample_bytree': 0.998138312716456, 'min_child_weight': 9}. Best is trial 11 with value: 0.8448068759349802.\n",
      "[I 2026-01-19 22:12:10,679] Trial 28 finished with value: 0.8298802887224114 and parameters: {'max_depth': 8, 'learning_rate': 0.013988394705530526, 'subsample': 0.6791266239550182, 'colsample_bytree': 0.7802609025776484, 'min_child_weight': 3}. Best is trial 11 with value: 0.8448068759349802.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Optuna optimization completed. Best R¬≤: 0.8448\n",
      "   üèãÔ∏è Training final model for AM-III-filtered_with_labels_k4...\n",
      "   üìä Making predictions for AM-III-filtered_with_labels_k4...\n",
      "   üé® Generating plots for AM-III-filtered_with_labels_k4...\n",
      "   üìà Calculating feature importance for AM-III-filtered_with_labels_k4...\n",
      "‚úÖ Completed: AM-III-filtered_with_labels_k4\n",
      "   Test R¬≤: 0.8900, MAE: 3.3834, RMSE: 4.8383\n",
      "   Results saved to: ./xgb-models/AM-III-filtered_with_labels_k4\n",
      "\n",
      "\n",
      "============================================================\n",
      "SUMMARY OF RESULTS\n",
      "============================================================\n",
      "                       dataset  n_train  n_test  best_r2  train_r2  test_r2  test_mae  test_rmse\n",
      "  AM-I-filtered_with_labels_k4     6118     681   0.8974    0.9810   0.9101    2.9681     3.8937\n",
      " AM-II-filtered_with_labels_k3     1650     186   0.8656    0.9968   0.8759    2.4358     3.2216\n",
      "AM-III-filtered_with_labels_k4     1089     123   0.8448    0.9907   0.8900    3.3834     4.8383\n",
      "\n",
      "üìã Summary saved to: ./xgb-models/summary_results.csv\n",
      "üìä Comparison plot saved to: ./xgb-models/performance_comparison.png\n",
      "\n",
      "‚ú® All processing completed! ‚ú®\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "XGBoost + Optuna 5-fold CV (R¬≤ mean) + iPhone Style Plotting\n",
    "Sequential processing for AM-I, AM-II, AM-III datasets\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import learning_curve\n",
    "# =========================================================\n",
    "# 1. Configuration\n",
    "# =========================================================\n",
    "DATA_FOLDER = './train_test_split'  # Modified: data files are in train_test_split folder\n",
    "OUTPUT_ROOT = os.path.join('./', 'xgb-models')\n",
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "\n",
    "FEATURE_COLS = ['MolWt', 'logP', 'TPSA', 'H_bond_donors', 'H_bond_acceptors']\n",
    "FP_COLS   = [f'col{i}'   for i in range(823)]\n",
    "MG_COLS   = [f'fp_{i}'   for i in range(1024)]\n",
    "ALL_FEATURES = FEATURE_COLS + FP_COLS + MG_COLS\n",
    "TARGET_COL   = 'UV_RT-s'\n",
    "\n",
    "# iPhone Style Color Palette\n",
    "IPHONE_COLORS = {\n",
    "    'scatter': '#007AFF',\n",
    "    'line':    '#AEAEB2',\n",
    "    'text':    '#000000'\n",
    "}\n",
    "\n",
    "# =========================================================\n",
    "# 2. Utility Functions\n",
    "# =========================================================\n",
    "def evaluate(y_true, y_pred):\n",
    "    \"\"\"Calculate evaluation metrics.\"\"\"\n",
    "    return {\n",
    "        'R2':  r2_score(y_true, y_pred),\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    }\n",
    "\n",
    "def iphone_style_ax(ax):\n",
    "    \"\"\"Apply iPhone-style aesthetics to matplotlib axes.\"\"\"\n",
    "    ax.tick_params(axis='both', direction='out', length=6, width=2, labelsize=16)\n",
    "    for spine in ['top', 'right', 'bottom', 'left']:\n",
    "        ax.spines[spine].set_visible(True)\n",
    "    ax.grid(False)\n",
    "\n",
    "def plot_scatter(y_true, y_pred, save_path):\n",
    "    \"\"\"Create scatter plot of true vs predicted values.\"\"\"\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = plt.gca()\n",
    "    iphone_style_ax(ax)\n",
    "    plt.scatter(y_true, y_pred, alpha=0.8, s=70, color=IPHONE_COLORS['scatter'])\n",
    "    lims = [min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())]\n",
    "    plt.plot(lims, lims, linestyle='--', color=IPHONE_COLORS['line'], linewidth=3)\n",
    "\n",
    "    r2  = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    plt.text(0.05, 0.95, f\"R¬≤ = {r2:.3g}\\nMAE = {mae:.3g}\",\n",
    "             transform=ax.transAxes, va='top', fontsize=16, color=IPHONE_COLORS['text'])\n",
    "\n",
    "    plt.xlabel(\"True Retention Time (s)\", fontsize=17)\n",
    "    plt.ylabel(\"Predicted Retention Time (s)\", fontsize=17)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_residuals(y_true, y_pred, save_path):\n",
    "    \"\"\"Create residual plot.\"\"\"\n",
    "    residuals = y_pred - y_true\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = plt.gca()\n",
    "    iphone_style_ax(ax)\n",
    "    plt.scatter(y_pred, residuals, alpha=0.8, s=70, color=IPHONE_COLORS['scatter'])\n",
    "    plt.axhline(y=0, linestyle='--', color=IPHONE_COLORS['line'], linewidth=3)\n",
    "\n",
    "    r2  = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    plt.text(0.05, 0.95, f\"R¬≤ = {r2:.3g}\\nMAE = {mae:.3g}\",\n",
    "             transform=ax.transAxes, va='top', fontsize=16, color=IPHONE_COLORS['text'])\n",
    "\n",
    "    plt.xlabel(\"Predicted Retention Time (s)\", fontsize=17)\n",
    "    plt.ylabel(\"Residuals (Predicted - True)\", fontsize=17)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_learning_curve(train_sizes, train_scores, val_scores, save_path):\n",
    "    \"\"\"Plot learning curve.\"\"\"\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = plt.gca()\n",
    "    iphone_style_ax(ax)\n",
    "\n",
    "    plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-',\n",
    "             color=IPHONE_COLORS['scatter'], linewidth=3, label='Train R¬≤')\n",
    "    plt.plot(train_sizes, np.mean(val_scores, axis=1), 'o-',\n",
    "             color=IPHONE_COLORS['line'], linewidth=3, label='Val R¬≤')\n",
    "\n",
    "    plt.xlabel('Training examples', fontsize=17)\n",
    "    plt.ylabel('R¬≤', fontsize=17)\n",
    "    plt.title('Learning Curve (XGBoost)', fontsize=17)\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def check_data_files(dataset_name):\n",
    "    \"\"\"Check if data files exist and their structure.\"\"\"\n",
    "    train_file = os.path.join(DATA_FOLDER, f\"{dataset_name}_train.csv\")\n",
    "    test_file = os.path.join(DATA_FOLDER, f\"{dataset_name}_test.csv\")\n",
    "    \n",
    "    print(f\"\\nüîç Checking data files for {dataset_name}:\")\n",
    "    print(f\"   Train file: {train_file}\")\n",
    "    print(f\"   Test file: {test_file}\")\n",
    "    \n",
    "    if not os.path.exists(train_file):\n",
    "        print(f\"   ‚ùå Train file not found!\")\n",
    "        return False\n",
    "    if not os.path.exists(test_file):\n",
    "        print(f\"   ‚ùå Test file not found!\")\n",
    "        return False\n",
    "    \n",
    "    # Check file structure\n",
    "    try:\n",
    "        train_df = pd.read_csv(train_file, nrows=1)\n",
    "        test_df = pd.read_csv(test_file, nrows=1)\n",
    "        \n",
    "        print(f\"   ‚úÖ Files found. Checking structure...\")\n",
    "        print(f\"   Train shape: {pd.read_csv(train_file).shape}\")\n",
    "        print(f\"   Test shape: {pd.read_csv(test_file).shape}\")\n",
    "        \n",
    "        # Check target column\n",
    "        if TARGET_COL not in train_df.columns:\n",
    "            print(f\"   ‚ùå Target column '{TARGET_COL}' not found in train data!\")\n",
    "            print(f\"   Available columns: {list(train_df.columns[:5])}...\")\n",
    "            return False\n",
    "        \n",
    "        # Check some feature columns\n",
    "        missing_features = [col for col in ALL_FEATURES[:10] if col not in train_df.columns]\n",
    "        if missing_features:\n",
    "            print(f\"   ‚ö†Ô∏è  Some features missing: {missing_features[:5]}...\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error reading files: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# =========================================================\n",
    "# 3. Main Processing Function\n",
    "# =========================================================\n",
    "def process_dataset(dataset_name):\n",
    "    \"\"\"Process a single dataset with XGBoost and Optuna optimization.\"\"\"\n",
    "    train_file = os.path.join(DATA_FOLDER, f\"{dataset_name}_train.csv\")\n",
    "    test_file  = os.path.join(DATA_FOLDER, f\"{dataset_name}_test.csv\")\n",
    "    \n",
    "    # First check if files exist\n",
    "    if not check_data_files(dataset_name):\n",
    "        print(f\"[SKIP] Cannot process {dataset_name} due to missing or invalid data files\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nüöÄ Processing dataset: {dataset_name}\")\n",
    "    output_dir = os.path.join(OUTPUT_ROOT, dataset_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # ---------- Load Data ----------\n",
    "    print(f\"   üì• Loading data from {DATA_FOLDER}...\")\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    test_df  = pd.read_csv(test_file)\n",
    "    \n",
    "    # Display data info\n",
    "    print(f\"   üìä Data loaded:\")\n",
    "    print(f\"     Train set: {len(train_df)} samples, {len(train_df.columns)} columns\")\n",
    "    print(f\"     Test set: {len(test_df)} samples, {len(test_df.columns)} columns\")\n",
    "    print(f\"     Target range (train): {train_df[TARGET_COL].min():.2f} - {train_df[TARGET_COL].max():.2f}\")\n",
    "    print(f\"     Target range (test): {test_df[TARGET_COL].min():.2f} - {test_df[TARGET_COL].max():.2f}\")\n",
    "\n",
    "    X_train = train_df[ALL_FEATURES]\n",
    "    y_train = train_df[TARGET_COL]\n",
    "    X_test  = test_df[ALL_FEATURES]\n",
    "    y_test  = test_df[TARGET_COL]\n",
    "\n",
    "    dtrain_full = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest       = xgb.DMatrix(X_test)\n",
    "\n",
    "    # ---------- Optuna Hyperparameter Optimization ----------\n",
    "    print(f\"   üîç Optimizing hyperparameters for {dataset_name}...\")\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'max_depth':        trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate':    trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "            'subsample':        trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'objective':        'reg:squarederror',\n",
    "            'tree_method':      'hist',\n",
    "            'eval_metric':      'rmse',\n",
    "            'seed':             42\n",
    "        }\n",
    "        cv = xgb.cv(params, dtrain_full,\n",
    "                    num_boost_round=1000,\n",
    "                    nfold=5,\n",
    "                    early_stopping_rounds=50,\n",
    "                    metrics='rmse',\n",
    "                    seed=42,\n",
    "                    verbose_eval=False)\n",
    "        # Calculate R¬≤ from RMSE: R¬≤ = 1 - RMSE¬≤ / Var(y)\n",
    "        rmse = cv['test-rmse-mean'].iloc[-1]\n",
    "        var_y = np.var(y_train)\n",
    "        r2 = 1 - (rmse ** 2) / var_y\n",
    "        return r2\n",
    "\n",
    "    study = optuna.create_study(direction='maximize',\n",
    "                                sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=30, n_jobs=26)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    final_params = {\n",
    "        'max_depth':        best_params['max_depth'],\n",
    "        'learning_rate':    best_params['learning_rate'],\n",
    "        'subsample':        best_params['subsample'],\n",
    "        'colsample_bytree': best_params['colsample_bytree'],\n",
    "        'min_child_weight': best_params['min_child_weight'],\n",
    "        'objective':        'reg:squarederror',\n",
    "        'tree_method':      'hist',\n",
    "        'eval_metric':      'rmse',\n",
    "        'seed':             42\n",
    "    }\n",
    "    \n",
    "    # Save Optuna trial results\n",
    "    optuna_log_df = pd.DataFrame([(t.number, t.value, t.params) for t in study.trials],\n",
    "                                 columns=['trial', 'val_r2_mean', 'params'])\n",
    "    optuna_log_df.to_csv(os.path.join(output_dir, f\"{dataset_name}_optuna_log.csv\"), index=False)\n",
    "    print(f\"   ‚úÖ Optuna optimization completed. Best R¬≤: {study.best_value:.4f}\")\n",
    "\n",
    "    # ---------- Train Final XGBoost Model ----------\n",
    "    print(f\"   üèãÔ∏è Training final model for {dataset_name}...\")\n",
    "    evals_result = {}\n",
    "    final_model = xgb.train(\n",
    "        final_params,\n",
    "        dtrain_full,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(dtrain_full, 'train')],\n",
    "        evals_result=evals_result,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    joblib.dump(final_model, os.path.join(output_dir, f\"{dataset_name}_final_model.pkl\"))\n",
    "\n",
    "    # ---------- Predictions ----------\n",
    "    print(f\"   üìä Making predictions for {dataset_name}...\")\n",
    "    y_pred       = final_model.predict(dtest)\n",
    "    y_train_pred = final_model.predict(dtrain_full)\n",
    "\n",
    "    train_metrics = evaluate(y_train, y_train_pred)\n",
    "    test_metrics  = evaluate(y_test,  y_pred)\n",
    "\n",
    "    # Save predictions\n",
    "    pd.DataFrame({'true': y_test, 'predicted': y_pred})\\\n",
    "      .to_csv(os.path.join(output_dir, f\"{dataset_name}_test_predictions.csv\"), index=False)\n",
    "\n",
    "    # Save evaluation metrics\n",
    "    pd.DataFrame([train_metrics, test_metrics], index=['train', 'test'])\\\n",
    "      .to_csv(os.path.join(output_dir, f\"{dataset_name}_evaluation_summary.csv\"))\n",
    "\n",
    "    # ---------- Generate Plots ----------\n",
    "    print(f\"   üé® Generating plots for {dataset_name}...\")\n",
    "    plot_scatter(y_test, y_pred,\n",
    "                 os.path.join(output_dir, f\"{dataset_name}_scatter.png\"))\n",
    "    plot_residuals(y_test, y_pred,\n",
    "                   os.path.join(output_dir, f\"{dataset_name}_residuals.png\"))\n",
    "\n",
    "    # Learning Curve\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        xgb.XGBRegressor(**{k: v for k, v in final_params.items() if k != 'objective'}),\n",
    "        X_train, y_train,\n",
    "        cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        scoring='r2', n_jobs=26,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5))\n",
    "    plot_learning_curve(train_sizes, train_scores, val_scores,\n",
    "                        os.path.join(output_dir, f\"{dataset_name}_learning_curve.png\"))\n",
    "\n",
    "    # ---------- Feature Importance ----------\n",
    "    print(f\"   üìà Calculating feature importance for {dataset_name}...\")\n",
    "    importance = final_model.get_score(importance_type='gain')\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': list(importance.keys()),\n",
    "        'importance': list(importance.values())\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    importance_df.to_csv(os.path.join(output_dir, f\"{dataset_name}_feature_importance.csv\"), index=False)\n",
    "\n",
    "    # Plot top 20 features\n",
    "    top_n = 20\n",
    "    if len(importance_df) > top_n:\n",
    "        plot_df = importance_df.head(top_n)\n",
    "    else:\n",
    "        plot_df = importance_df\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    ax = plt.gca()\n",
    "    iphone_style_ax(ax)\n",
    "    colors = plt.cm.Blues(np.linspace(0.3, 1, len(plot_df)))\n",
    "    ax.barh(range(len(plot_df)), plot_df['importance'], color=colors)\n",
    "    ax.set_yticks(range(len(plot_df)))\n",
    "    ax.set_yticklabels(plot_df['feature'], fontsize=10)\n",
    "    ax.set_xlabel('Feature Importance (Gain)', fontsize=14)\n",
    "    ax.set_title(f'Top {len(plot_df)} Feature Importance - {dataset_name}', fontsize=16)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"{dataset_name}_feature_importance.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"‚úÖ Completed: {dataset_name}\")\n",
    "    print(f\"   Test R¬≤: {test_metrics['R2']:.4f}, MAE: {test_metrics['MAE']:.4f}, RMSE: {test_metrics['RMSE']:.4f}\")\n",
    "    print(f\"   Results saved to: {output_dir}\\n\")\n",
    "    \n",
    "    # Return metrics for summary\n",
    "    return {\n",
    "        'dataset': dataset_name,\n",
    "        'best_r2': study.best_value,\n",
    "        'test_r2': test_metrics['R2'],\n",
    "        'test_mae': test_metrics['MAE'],\n",
    "        'test_rmse': test_metrics['RMSE'],\n",
    "        'train_r2': train_metrics['R2'],\n",
    "        'n_train': len(train_df),\n",
    "        'n_test': len(test_df)\n",
    "    }\n",
    "\n",
    "# =========================================================\n",
    "# 4. Sequential Processing for AM Datasets\n",
    "# =========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Define target datasets for sequential processing\n",
    "    TARGET_DATASETS = ['AM-I-filtered_with_labels_k4', 'AM-II-filtered_with_labels_k3', 'AM-III-filtered_with_labels_k4']\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"XGBoost Sequential Processing for AM Datasets\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìÅ Data folder: {os.path.abspath(DATA_FOLDER)}\")\n",
    "    print(f\"üìÅ Output folder: {os.path.abspath(OUTPUT_ROOT)}\")\n",
    "    \n",
    "    # Check if data folder exists\n",
    "    if not os.path.exists(DATA_FOLDER):\n",
    "        print(f\"\\n‚ùå ERROR: Data folder '{DATA_FOLDER}' does not exist!\")\n",
    "        print(f\"Please create the folder and place your data files there.\")\n",
    "        print(f\"Expected files: AM-I_train.csv, AM-I_test.csv, etc.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # List files in data folder\n",
    "    print(f\"\\nüìã Files in data folder:\")\n",
    "    data_files = os.listdir(DATA_FOLDER)\n",
    "    csv_files = [f for f in data_files if f.endswith('.csv')]\n",
    "    for file in sorted(csv_files):\n",
    "        file_path = os.path.join(DATA_FOLDER, file)\n",
    "        file_size = os.path.getsize(file_path) / (1024*1024)  # MB\n",
    "        print(f\"   {file} ({file_size:.1f} MB)\")\n",
    "    \n",
    "    # Sequential processing (no parallelization)\n",
    "    all_metrics = []\n",
    "    for dataset in TARGET_DATASETS:\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        try:\n",
    "            metrics = process_dataset(dataset)\n",
    "            if metrics:\n",
    "                all_metrics.append(metrics)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {dataset}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Print summary table\n",
    "    if all_metrics:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"SUMMARY OF RESULTS\")\n",
    "        print(\"=\" * 60)\n",
    "        summary_df = pd.DataFrame(all_metrics)\n",
    "        # Reorder columns for better readability\n",
    "        summary_df = summary_df[['dataset', 'n_train', 'n_test', 'best_r2', 'train_r2', 'test_r2', 'test_mae', 'test_rmse']]\n",
    "        print(summary_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "        \n",
    "        # Save summary\n",
    "        summary_df.to_csv(os.path.join(OUTPUT_ROOT, 'summary_results.csv'), index=False)\n",
    "        print(f\"\\nüìã Summary saved to: {os.path.join(OUTPUT_ROOT, 'summary_results.csv')}\")\n",
    "        \n",
    "        # Create a simple visualization of results comparison\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = plt.gca()\n",
    "        iphone_style_ax(ax)\n",
    "        \n",
    "        x_pos = np.arange(len(all_metrics))\n",
    "        width = 0.35\n",
    "        \n",
    "        train_r2 = [m['train_r2'] for m in all_metrics]\n",
    "        test_r2 = [m['test_r2'] for m in all_metrics]\n",
    "        \n",
    "        ax.bar(x_pos - width/2, train_r2, width, label='Train R¬≤', color=IPHONE_COLORS['scatter'])\n",
    "        ax.bar(x_pos + width/2, test_r2, width, label='Test R¬≤', color=IPHONE_COLORS['line'])\n",
    "        \n",
    "        ax.set_xlabel('Dataset', fontsize=14)\n",
    "        ax.set_ylabel('R¬≤ Score', fontsize=14)\n",
    "        ax.set_title('Model Performance Comparison', fontsize=16)\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels([m['dataset'] for m in all_metrics])\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_ROOT, 'performance_comparison.png'), dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"üìä Comparison plot saved to: {os.path.join(OUTPUT_ROOT, 'performance_comparison.png')}\")\n",
    "    \n",
    "    print(\"\\n‚ú® All processing completed! ‚ú®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2ef520",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbed747e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fusion_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
