{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8ea88e2",
   "metadata": {},
   "source": [
    "#  lgb model of AM-II and AM-III by transfer learning of from AM-I "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3e9d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import joblib\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Set LightGBM thread count to 26\n",
    "os.environ['LIGHTGBM_NUM_THREADS'] = '10'\n",
    "\n",
    "# -------------------- Global Settings --------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Dataset directory set to current directory\n",
    "DATA_DIR = \"./train_test_split\"\n",
    "\n",
    "# Pretrained model directory set to ./lgb-models\n",
    "PRETRAIN_MODEL_DIR = \"./lgb-models\"\n",
    "\n",
    "# Modified model save directory to ./lgb-TL\n",
    "OUTPUT_FOLDER = './'\n",
    "MODEL_DIR = os.path.join(OUTPUT_FOLDER, 'lgb-TL')\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------- Logging Configuration --------------------\n",
    "# Modified: Save log file to ./lgb-TL directory\n",
    "log_file_path = os.path.join(MODEL_DIR, \"lgb_transfer_from_AM-I.log\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_file_path,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    filemode=\"a\"\n",
    ")\n",
    "\n",
    "TARGET_DATASETS = [\n",
    "    \"AM-III-filtered_with_labels_k4\",\n",
    "    \"AM-II-filtered_with_labels_k3\",\n",
    "]\n",
    "\n",
    "PRETRAIN_TAG = \"AM-I-filtered_with_labels_k4\"\n",
    "# Keep original tag without replacing special characters\n",
    "PRETRAIN_SAFE_TAG = PRETRAIN_TAG\n",
    "\n",
    "# Pretrained model paths\n",
    "PRETRAIN_MODEL_PATH = os.path.join(PRETRAIN_MODEL_DIR, f\"{PRETRAIN_SAFE_TAG}_model.joblib\")\n",
    "PRETRAIN_FEATURE_PATH = os.path.join(PRETRAIN_MODEL_DIR, f\"{PRETRAIN_SAFE_TAG}_feature_list.pkl\")\n",
    "PRETRAIN_IMPUTER_PATH = os.path.join(PRETRAIN_MODEL_DIR, f\"{PRETRAIN_SAFE_TAG}_imputer.pkl\")\n",
    "PRETRAIN_METRICS_PATH = os.path.join(PRETRAIN_MODEL_DIR, f\"{PRETRAIN_SAFE_TAG}_metrics.json\")\n",
    "\n",
    "IPHONE_COLORS = {\n",
    "    \"scatter\": \"#007AFF\",\n",
    "    \"line\": \"#FF3B30\",\n",
    "    \"residual\": \"#34C759\",\n",
    "    \"text\": \"#1C1C1E\"\n",
    "}\n",
    "\n",
    "def safe_tag(tag: str) -> str:\n",
    "    \"\"\"Return the tag as-is without replacing special characters.\"\"\"\n",
    "    return tag\n",
    "\n",
    "\n",
    "# -------------------- Plotting Functions --------------------\n",
    "def plot_scatter_and_residuals(y_true, y_pred, save_folder, base_name):\n",
    "    try:\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "        # Scatter plot\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        ax = plt.gca()\n",
    "        ax.tick_params(axis='both', direction='out', length=6, width=1.2, color='black')\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(True)\n",
    "            spine.set_color('black')\n",
    "        plt.grid(False)\n",
    "        plt.scatter(y_true, y_pred, alpha=0.6, color=IPHONE_COLORS['scatter'], edgecolor='k', s=50)\n",
    "        xymin, xymax = float(np.min(y_true)), float(np.max(y_true))\n",
    "        plt.plot([xymin, xymax], [xymin, xymax], linestyle='--', color=IPHONE_COLORS['line'], linewidth=1)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        plt.xlabel(\"True Retention Time (s)\")\n",
    "        plt.ylabel(\"Predicted Retention Time (s)\")\n",
    "        plt.text(0.05, 0.95, f\"RÂ² = {r2:.3g}\\nMAE = {mae:.3g}\", transform=ax.transAxes, va='top', ha='left', fontsize=10, color=IPHONE_COLORS['text'])\n",
    "        pad = 0.1 * (xymax - xymin + 1e-9)\n",
    "        ax.set_ylim([xymin - pad, xymax + pad])\n",
    "        ax.set_facecolor('white')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_folder, f\"{base_name}_scatter.png\"), dpi=600)\n",
    "        plt.close()\n",
    "\n",
    "        # Residual plot\n",
    "        residuals = y_pred - y_true\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        ax = plt.gca()\n",
    "        ax.tick_params(axis='both', direction='out', length=6, width=1.2, color='black')\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(True)\n",
    "            spine.set_color('black')\n",
    "        plt.grid(False)\n",
    "        plt.scatter(y_pred, residuals, alpha=0.6, color=IPHONE_COLORS['scatter'], edgecolor='k', s=50)\n",
    "        plt.axhline(y=0, linestyle='--', color=IPHONE_COLORS['line'], linewidth=1)\n",
    "        plt.xlabel(\"Predicted Retention Time (s)\")\n",
    "        plt.ylabel(\"Residuals (Predicted - True)\")\n",
    "        rmin, rmax = float(np.min(residuals)), float(np.max(residuals))\n",
    "        pad = 0.1 * (rmax - rmin + 1e-9)\n",
    "        ax.set_ylim([rmin - pad, rmax + pad])\n",
    "        ax.set_facecolor('white')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_folder, f\"{base_name}_residuals.png\"), dpi=600)\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[{base_name}] Plotting failed: {str(e)}\", exc_info=True)\n",
    "\n",
    "\n",
    "def plot_optuna_rmse(study, save_path):\n",
    "    try:\n",
    "        best_rmses = [t.value for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "        if not best_rmses:\n",
    "            return\n",
    "        cumulative_best = np.minimum.accumulate(best_rmses)\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(cumulative_best, marker='o', linestyle='-', color='blue')\n",
    "        plt.xlabel('Trial')\n",
    "        plt.ylabel('Best RMSE so far')\n",
    "        plt.title('Optuna RMSE Convergence')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=600)\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to plot Optuna RMSE curve: {str(e)}\", exc_info=True)\n",
    "\n",
    "\n",
    "# -------------------- Pretrained Asset Loading --------------------\n",
    "def load_pretrained_assets():\n",
    "    if not (os.path.isfile(PRETRAIN_MODEL_PATH) and os.path.isfile(PRETRAIN_FEATURE_PATH) and os.path.isfile(PRETRAIN_IMPUTER_PATH)):\n",
    "        raise FileNotFoundError(\"Pretrained assets not found\")\n",
    "    model = joblib.load(PRETRAIN_MODEL_PATH)\n",
    "    feature_cols = joblib.load(PRETRAIN_FEATURE_PATH)\n",
    "    imputer = joblib.load(PRETRAIN_IMPUTER_PATH)\n",
    "    pretrain_params = {}\n",
    "    if os.path.isfile(PRETRAIN_METRICS_PATH):\n",
    "        try:\n",
    "            with open(PRETRAIN_METRICS_PATH, \"r\") as f:\n",
    "                pretrain_params = json.load(f).get(\"best_params\", {})\n",
    "        except Exception:\n",
    "            pass\n",
    "    return model, feature_cols, imputer, pretrain_params\n",
    "\n",
    "\n",
    "# -------------------- Data Checking --------------------\n",
    "def check_and_extract(df: pd.DataFrame, feature_cols: list, target_col: str):\n",
    "    missing = [c for c in feature_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Data missing the following feature columns: {missing}\")\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"Missing target column '{target_col}'\")\n",
    "    X = df[feature_cols].values\n",
    "    y = df[target_col].values\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# -------------------- Fine-tuning with Optuna --------------------\n",
    "def finetune_with_optuna(tag_raw, pretrain_model, feature_cols, imputer, pretrain_params,\n",
    "                         target_col=\"UV_RT-s\", n_trials=40, n_splits=5,\n",
    "                         new_trees=50):\n",
    "    tag = safe_tag(tag_raw)  # Returns original tag unchanged\n",
    "    train_path = os.path.join(DATA_DIR, f\"{tag_raw}_train.csv\")\n",
    "    test_path = os.path.join(DATA_DIR, f\"{tag_raw}_test.csv\")\n",
    "    \n",
    "    # Modified: Create dataset directory with dataset name only\n",
    "    dataset_dir = os.path.join(MODEL_DIR, f\"{tag}\")\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    if not os.path.isfile(train_path) or not os.path.isfile(test_path):\n",
    "        msg = f\"Cannot find train/test CSV for {tag_raw}\"\n",
    "        logging.warning(msg)\n",
    "        return {\"tag\": tag_raw, \"status\": \"warn\", \"message\": msg}\n",
    "\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    X_all, y_all = check_and_extract(train_df, feature_cols, target_col)\n",
    "    X_test, y_test = check_and_extract(test_df, feature_cols, target_col)\n",
    "\n",
    "    X_all = imputer.transform(X_all)\n",
    "    X_test = imputer.transform(X_test)\n",
    "\n",
    "    # -------------------- Safe Parameter Handling --------------------\n",
    "    base_params = pretrain_model.get_params()\n",
    "    base_params.pop(\"n_estimators\", None)\n",
    "    base_params.pop(\"random_state\", None)\n",
    "    if pretrain_params:\n",
    "        pretrain_params.pop(\"n_estimators\", None)\n",
    "        pretrain_params.pop(\"random_state\", None)\n",
    "        base_params.update(pretrain_params)\n",
    "\n",
    "    # -------------------- Multi-stage Parameter Tuning --------------------\n",
    "    def objective(trial, stage=\"coarse\"):\n",
    "        params = base_params.copy()\n",
    "        if stage == \"coarse\":\n",
    "            params.update({\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.002, 0.1, log=True),\n",
    "                \"num_leaves\": trial.suggest_int(\"num_leaves\", 16, 128),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "                \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 50),\n",
    "                \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            })\n",
    "        else:  # fine\n",
    "            params.update({\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.05, log=True),\n",
    "                \"num_leaves\": trial.suggest_int(\"num_leaves\", max(16, base_params.get(\"num_leaves\", 31)-16),\n",
    "                                               min(128, base_params.get(\"num_leaves\", 31)+16)),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", max(3, base_params.get(\"max_depth\", 7)-3),\n",
    "                                              min(12, base_params.get(\"max_depth\", 7)+3)),\n",
    "            })\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "        rmses = []\n",
    "        for train_idx, valid_idx in kf.split(X_all):\n",
    "            X_tr, X_val = X_all[train_idx], X_all[valid_idx]\n",
    "            y_tr, y_val = y_all[train_idx], y_all[valid_idx]\n",
    "\n",
    "            model = lgb.LGBMRegressor(\n",
    "                **params,\n",
    "                n_estimators=pretrain_model.n_estimators + new_trees,\n",
    "                random_state=SEED\n",
    "            )\n",
    "            model.fit(\n",
    "                X_tr, y_tr,\n",
    "                init_model=pretrain_model,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                eval_metric=\"rmse\",\n",
    "                callbacks=[early_stopping(stopping_rounds=50)]\n",
    "            )\n",
    "            y_pred = model.predict(X_val)\n",
    "            rmses.append(np.sqrt(mean_squared_error(y_val, y_pred)))\n",
    "        return float(np.mean(rmses))\n",
    "\n",
    "    # -------------------- Optuna Parameter Tuning --------------------\n",
    "    study = optuna.create_study(direction=\"minimize\", pruner=MedianPruner(n_startup_trials=5))\n",
    "    study.optimize(lambda t: objective(t, stage=\"coarse\"), n_trials=n_trials, show_progress_bar=True)\n",
    "    plot_optuna_rmse(study, os.path.join(dataset_dir, f\"{tag}_coarse_optuna.png\"))\n",
    "\n",
    "    best_params = base_params.copy()\n",
    "    best_params.update(study.best_params)\n",
    "    fine_model = lgb.LGBMRegressor(\n",
    "        **best_params,\n",
    "        n_estimators=pretrain_model.n_estimators + new_trees,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    fine_model.fit(X_all, y_all, init_model=pretrain_model)\n",
    "\n",
    "    # -------------------- Scratch Training Control Model --------------------\n",
    "    scratch_model = lgb.LGBMRegressor(\n",
    "        **best_params,\n",
    "        n_estimators=pretrain_model.n_estimators + new_trees,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    scratch_model.fit(X_all, y_all)\n",
    "\n",
    "    y_pred_test_fine = fine_model.predict(X_test)\n",
    "    y_pred_test_scratch = scratch_model.predict(X_test)\n",
    "\n",
    "    rmse_fine = np.sqrt(mean_squared_error(y_test, y_pred_test_fine))\n",
    "    r2_fine = r2_score(y_test, y_pred_test_fine)\n",
    "    mae_fine = mean_absolute_error(y_test, y_pred_test_fine)\n",
    "\n",
    "    rmse_scratch = np.sqrt(mean_squared_error(y_test, y_pred_test_scratch))\n",
    "    r2_scratch = r2_score(y_test, y_pred_test_scratch)\n",
    "    mae_scratch = mean_absolute_error(y_test, y_pred_test_scratch)\n",
    "\n",
    "    plot_scatter_and_residuals(y_test, y_pred_test_fine, dataset_dir, f\"{tag}_finetuned\")\n",
    "    plot_scatter_and_residuals(y_test, y_pred_test_scratch, dataset_dir, f\"{tag}_scratch\")\n",
    "\n",
    "    # -------------------- Save Models and Parameters --------------------\n",
    "    fine_model_path = os.path.join(dataset_dir, f\"{tag}_finetuned_model.joblib\")\n",
    "    scratch_model_path = os.path.join(dataset_dir, f\"{tag}_scratch_model.joblib\")\n",
    "    feature_path = os.path.join(dataset_dir, f\"{tag}_feature_list.pkl\")\n",
    "    imputer_path = os.path.join(dataset_dir, f\"{tag}_imputer.pkl\")\n",
    "    metrics_path = os.path.join(dataset_dir, f\"{tag}_metrics.json\")\n",
    "\n",
    "    joblib.dump(fine_model, fine_model_path)\n",
    "    joblib.dump(scratch_model, scratch_model_path)\n",
    "    joblib.dump(feature_cols, feature_path)\n",
    "    joblib.dump(imputer, imputer_path)\n",
    "\n",
    "    metrics = {\n",
    "        \"finetuned\": {\n",
    "            \"rmse\": rmse_fine,\n",
    "            \"r2\": r2_fine,\n",
    "            \"mae\": mae_fine\n",
    "        },\n",
    "        \"scratch\": {\n",
    "            \"rmse\": rmse_scratch,\n",
    "            \"r2\": r2_scratch,\n",
    "            \"mae\": mae_scratch\n",
    "        },\n",
    "        \"best_params\": best_params\n",
    "    }\n",
    "\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    logging.info(f\"[{tag_raw}] Fine-tuning completed. Finetuned RMSE={rmse_fine:.4f}, Scratch RMSE={rmse_scratch:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"tag\": tag_raw,\n",
    "        \"status\": \"ok\",\n",
    "        \"metrics\": metrics,\n",
    "        \"fine_model_path\": fine_model_path,\n",
    "        \"scratch_model_path\": scratch_model_path\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------- Main Workflow --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        pretrain_model, feature_cols, imputer, pretrain_params = load_pretrained_assets()\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(str(e))\n",
    "        raise\n",
    "\n",
    "    results = []\n",
    "    for target_dataset in TARGET_DATASETS:\n",
    "        res = finetune_with_optuna(target_dataset, pretrain_model, feature_cols, imputer, pretrain_params,\n",
    "                                   n_trials=40, n_splits=5, new_trees=50)\n",
    "        results.append(res)\n",
    "\n",
    "    results_path = os.path.join(MODEL_DIR, \"finetune_results.json\")\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    logging.info(\"All dataset fine-tuning completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8ca715",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ec9f25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fusion_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
